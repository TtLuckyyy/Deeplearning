# 深度学习与神经网络：利用LSTM模型生成唐诗

##  1. RNN 、LSTM、GRU模型

### RNN

RNN（循环神经网络）是一种能够处理序列数据的神经网络，其核心特点是**利用隐藏状态（Hidden State）存储和传递历史信息**，从而捕捉序列的时间动态信息。

**主要组成部分**：

- **隐藏状态**：用于存储序列信息，实现“记忆”功能。
- **输入权重**：决定当前输入对隐藏状态的影响。
- **隐藏权重**：决定前一个隐藏状态对当前隐藏状态的影响。

**工作流程**：

1. 接收当前时间步的输入，并结合上一个隐藏状态进行计算。
2. 更新隐藏状态，存储历史信息。
3. 生成当前时间步的输出。
4. 迭代执行，直到处理完整个序列。

RNN适用于**自然语言处理、语音识别、时间序列分析**等任务，但**容易面临梯度消失问题**，可通过LSTM/GRU等变体优化。

### LSTM

LSTM（长短期记忆网络）是RNN的变体，**通过门控机制解决梯度消失问题**，从而更好地捕捉**长期依赖关系**。

**主要组件**：

- **细胞状态**：存储长期记忆。
- **输入门**：决定哪些信息更新到细胞状态。
- **遗忘门**：决定哪些信息从细胞状态中删除。
- **输出门**：决定细胞状态中的哪些信息需要输出。

**工作流程**：

1. **输入门**更新细胞状态。
2. **遗忘门**决定遗忘哪些信息。
3. **更新细胞状态**，结合输入和遗忘信息。
4. **输出门**决定最终输出与隐藏状态。
5. 迭代执行，处理完整个序列。

LSTM广泛应用于**自然语言处理、语音识别、机器翻译**等任务，能够**捕捉长期依赖，提高序列建模能力**。

### GRU

**GRU（门控循环单元，Gated Recurrent Unit）**是一种改进的循环神经网络（RNN）架构，旨在解决传统RNN的梯度消失问题。它比LSTM结构更简单，参数较少，但仍能有效地处理长时间依赖的问题。

**GRU的核心组件：**

1. **更新门（Update Gate）**：决定了当前时间步的隐藏状态应该保留多少来自上一时间步的记忆。
2. **重置门（Reset Gate）**：控制当前输入与上一隐藏状态的结合方式，决定遗忘多少过去的记忆。
3. **候选隐藏状态（Candidate Hidden State）**：基于当前输入和上一隐藏状态计算的“新记忆”。

**GRU的工作流程：**

1. **更新门**：决定当前隐藏状态是保留上一时刻的记忆还是加入新的记忆。
2. **重置门**：决定当前隐藏状态与前一时刻隐藏状态的结合方式。
3. **候选隐藏状态**：计算新的记忆。
4. **最终隐藏状态**：更新隐藏状态为上一时刻的状态和候选状态的加权平均。

GRU的优点在于其计算效率和较少的参数，适用于需要处理长时序列数据的任务，如自然语言处理和语音识别。

##  2. 诗歌生成的过程

本次实验主要针对Pytorch版本进行了详细的研究和构建，对于tensorflow版本的代码只是把必要的函数填写补充上了，其余的函数没有做过多探究，因此这里只对Pytorch版本的代码进行详细分析。

### ==tensorflow版本==

### 1. **数据处理模块**

- `process_dataset(fileName)`: 读取诗歌数据，进行分词、编码，并构建词表 (`word2id`, `id2word`)。
- `poem_dataset()`: 使用 `tf.data.Dataset` 处理数据，生成适用于训练的 TensorFlow 数据集（包括 `padded_batch` 和 `map` 操作）。

### 2. **模型定义模块**

- `myRNNModel(keras.Model)`: 自定义 RNN 语言模型，包括：
  - 嵌入层 (`Embedding`)
  - RNN 计算单元 (`SimpleRNNCell`)
  - 全连接层 (`Dense`) 预测下一个字符
- `call(self, inp_ids)`: 前向传播计算 logits
- `get_next_token(self, x, state)`: 生成下一个字符

### 3. **损失计算模块**

- `mkMask(input_tensor, maxLen)`: 生成掩码，用于处理不同长度的序列。
- `reduce_avg(reduce_target, lengths, dim)`: 计算序列的平均损失。
- `compute_loss(logits, labels, seqlen)`: 计算 `sparse_softmax_cross_entropy` 作为训练损失。

### 4. **训练模块**

- `train_one_step(model, optimizer, x, y, seqlen)`: 计算梯度并执行优化更新参数。
- `train(epoch, model, optimizer, ds)`: 遍历训练数据集，输出每 500 步的损失。

### 5. **训练过程**

- 定义优化器 (`Adam`)
- 加载数据集 (`poem_dataset()`)
- 初始化模型 (`myRNNModel`)
- 进行 10 轮训练 (`train`)

### 6. **诗歌生成模块**

- `gen_sentence()`: 从给定的起始字符生成 50 个字符的诗句，采用 `argmax` 选择最可能的下一个字符。
- 多次调用 `gen_sentence()` 生成诗歌，观察不同起始字符（如 `"日"`, `"红"`, `"山"`, `"夜"`, `"湖"`）的输出。


### ==Pytorch版本==

### 1. **数据处理部分**

数据处理部分主要负责加载并处理诗歌数据，使其适合输入到神经网络中。

- **`process_poems1` 和 `process_poems2`**： 这两个函数的作用是从指定的文件（`poems.txt`）中加载诗歌，去除一些无用字符（如空格、标点符号等），并将每个字映射成对应的整数索引。
  - 首先，读取文件中的每一行诗歌，并去除无效字符。
  - 然后，将每个字映射为一个索引，并将这些索引保存为二维数组 `poems_vector`，其中每行代表一首诗，行内是按顺序排列的字的索引。
  - 统计诗歌中所有字的频率，并为每个字分配一个索引，建立 `word_int_map`（字到索引的映射）。

### 2. **批次生成函数**

- **`generate_batch`**： 该函数生成模型训练所需的批次数据。它将 `poems_vec` 分割成多个小批次（每批次 `batch_size` 个诗歌），并将每个批次的数据拆分为输入数据 `x` 和目标数据 `y`。
  - 输入 `x` 是一首诗的字的索引序列。
  - 目标 `y` 是将每首诗的每个字向右移动一个位置后的索引序列。

### 3. **RNN模型和训练**

- **`run_training`**： 该函数实现了模型的训练过程。主要步骤如下：
  - 使用 `process_poems1` 处理诗歌数据，并生成批次数据。
  - 定义一个简单的词嵌入层 `word_embedding` 和一个RNN模型（`rnn_model`），并初始化优化器和损失函数。
  - 迭代多个epoch（此代码中为30个epoch），对于每个批次，计算损失并通过反向传播更新模型参数。
  - 训练过程中，梯度裁剪（`torch.nn.utils.clip_grad_norm`）用于避免梯度爆炸，定期保存训练的模型。

### 4. **生成诗歌**

- **`gen_poem`**： 该函数用于生成诗歌。给定一个起始字（如“日”），模型会逐字预测并生成诗歌。其步骤如下：
  - 首先，将起始字转为对应的索引，作为输入传入模型。
  - 模型根据当前输入的诗歌（按字的索引表示）预测下一个字。
  - 然后将预测出的字追加到诗歌中，重复此过程直到生成结束标记（`end_token`）或生成的诗歌长度达到最大限制（30个字）。
- **`to_word`**： 将预测结果（索引）转换为相应的字。
- **`pretty_print_poem`**： 格式化打印生成的诗歌，使其更加工整，每一句诗后面添加“。”。

### 5. **模型推理**

- 使用训练好的模型（通过 `rnn_model.load_state_dict` 加载已保存的模型），输入一个起始字，调用 `gen_poem` 来生成一首完整的诗。

## 3. 结果截图

训练过程的截图

![image-20250325180847055](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20250325180847055.png)


生成诗歌的结果

![image-20250325180704903](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\image-20250325180704903.png)
